{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of Models, functions, dependencies and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "SAVE_DIR = Path(\"segmentation/validation_results\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LR = 0.0023337072137759097\n",
    "BATCH_SIZE=  6\n",
    "# DROPOUT_P = 0.43498360821913545 \n",
    "# DICE_WEIGHT = 0.35695347056147286\n",
    "PRED_THRESH = 0.42610762019278425\n",
    "OPTIM= \"Adam\"\n",
    "EPOCHS = 10\n",
    "NUM_SAMPLES = 30\n",
    "\n",
    "# Paths\n",
    "data_dir = Path(r\"/Users/petrahlavinova/School/DP/d/data\")\n",
    "mask_dir = Path(r\"/Users/petrahlavinova/School/DP/d/generated_masks5\")\n",
    "\n",
    "\n",
    "# Dataset Definition\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, data_dir, mask_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(data_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_dir / self.images[idx]\n",
    "        mask_path = self.mask_dir / self.masks[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "import random\n",
    "\n",
    "class AugmentedSegmentationDataset(Dataset):\n",
    "    def __init__(self, data_dir, mask_dir, transform=None, augment=True):\n",
    "        \"\"\"\n",
    "        Dataset class for segmentation with optional augmentation.\n",
    "        Args:\n",
    "            data_dir: Path to the images.\n",
    "            mask_dir: Path to the masks.\n",
    "            transform: Transformations to apply to images and masks.\n",
    "            augment: Whether to apply augmentations (default True).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.images = sorted(os.listdir(data_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images) * (4 if self.augment else 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine the original image and rotation to apply\n",
    "        original_idx = idx // 4\n",
    "        rotation = idx % 4 * 90  # Rotate by 0, 90, 180, or 270 degrees\n",
    "        \n",
    "        img_path = self.data_dir / self.images[original_idx]\n",
    "        mask_path = self.mask_dir / self.masks[original_idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "        \n",
    "        # Apply rotation\n",
    "        image = image.rotate(rotation)\n",
    "        mask = mask.rotate(rotation)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "# Transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),  # Resize images and masks\n",
    "    T.ToTensor(),           # Convert to PyTorch tensors\n",
    "    # T.RandomRotation(30),  # Náhodné otočenie\n",
    "    # T.RandomHorizontalFlip(),  # Horizontálne zrkadlenie\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "dataset = SegmentationDataset(data_dir, mask_dir, transform=transform)\n",
    "\n",
    "# Split Dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-7):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)  # Sigmoid pre škálovanie do rozsahu [0, 1]\n",
    "        target = target.float()\n",
    "\n",
    "        intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "        union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight_dice=0.5, weight_bce=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_bce = weight_bce\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        bce = self.bce_loss(pred, target)\n",
    "        return self.weight_dice * dice + self.weight_bce * bce\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2 with dropout\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNetWithDropout(nn.Module):\n",
    "    \"\"\"Modified U-Net with dropout layers\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetWithDropout, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(128, 64)\n",
    "        \n",
    "        # Final output\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Decoder\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = self.decoder4(torch.cat((dec4, enc4), dim=1))\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = self.decoder3(torch.cat((dec3, enc3), dim=1))\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = self.decoder2(torch.cat((dec2, enc2), dim=1))\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.decoder1(torch.cat((dec1, enc1), dim=1))\n",
    "        \n",
    "        return self.final_conv(dec1)\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Počet epoch bez zlepšenia, po ktorých sa tréning zastaví.\n",
    "            min_delta (float): Minimálne zlepšenie metriky, aby sa počítalo ako zlepšenie.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def check_stop(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "\n",
    "def dice_score(pred, target, threshold=PRED_THRESH):\n",
    "\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()  # Binarize predictions\n",
    "    target = target.float()\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "    union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))\n",
    "    dice = (2 * intersection) / (union + 1e-7)  # Avoid division by zero\n",
    "    \n",
    "    return dice.mean().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetWithDropout(in_channels=3, out_channels=1)  # RGB images, binary masks\n",
    "model.to(device)  \n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()  # Binary segmentation task\n",
    "criterion = CombinedLoss(weight_dice=0.7, weight_bce=0.3)  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmentation(image, mask, prediction, epoch, idx):\n",
    "\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # Convert image to HWC format\n",
    "    mask = mask.cpu().numpy()\n",
    "    prediction = (torch.sigmoid(prediction) > 0.5).float().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction, cmap='gray')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "model = UNetWithDropout(in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(\"/segmentation/unet_segmentationSE.pth\"))\n",
    "\n",
    "\n",
    "# Training and Validation Loop\n",
    "epochs = EPOCHS\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    # train_loss = 0\n",
    "    # for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "    #     images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "    #     # Forward pass\n",
    "    #     outputs = model(images)\n",
    "    #     loss = criterion(outputs, masks)\n",
    "        \n",
    "    #     # Backward pass\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    #     train_loss += loss.item()\n",
    "    \n",
    "    # avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_dice = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for idx, (images, masks) in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_dice += dice_score(outputs, masks)\n",
    "\n",
    "            # Visualization for the first batch of each epoch\n",
    "            if idx == 0:\n",
    "                for i in range(min(3, len(images))):  # Visualize up to 3 images per epoch\n",
    "                    plot_segmentation(images[i], masks[i][0], outputs[i][0], epoch, idx)\n",
    "\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_dice = val_dice / len(val_loader)\n",
    "    \n",
    "    # Print Epoch Summary\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "        #   f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Dice Score: {avg_val_dice:.4f}\")\n",
    "    \n",
    "    if early_stopping.check_stop(avg_val_loss):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        # torch.save(model.state_dict(), \"unet_segmentationDropout.pth\")\n",
    "        break\n",
    "\n",
    "# Save the Model\n",
    "# torch.save(model.state_dict(), \"unet_segmentationDropout.pth\")\n",
    "\n",
    "# Uncertainty Estimation starts here:\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWithDropout(in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(\"/segmentation/unet_segmentationSE.pth\"))\n",
    "\n",
    "# Uncertainty Estimation starts here:\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(\n",
    "    model, \n",
    "    input_image, \n",
    "    num_samples=20, \n",
    "    return_entropy=False, \n",
    "    return_std=False,\n",
    "    return_iqr=False,\n",
    "    return_range95=False,\n",
    "    return_samples=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Monte Carlo Dropout inference for segmentation uncertainty.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model with dropout layers.\n",
    "        input_image: Input tensor, shape [B, C, H, W] or [C, H, W].\n",
    "        num_samples: Number of MC samples.\n",
    "        return_entropy: If True, returns pixel-wise entropy map.\n",
    "        return_std: If True, returns pixel-wise standard deviation map.\n",
    "        return_iqr: If True, returns pixel-wise IQR map.\n",
    "        return_range95: If True, returns pixel-wise 95% range map.\n",
    "        return_samples: If True, returns raw probability samples.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            mean_pred (np.ndarray): Mean probability map, shape [H,W].\n",
    "            var_map (np.ndarray): Variance map, shape [H,W].\n",
    "            std_map (np.ndarray, optional): Std dev map, shape [H,W].\n",
    "            entropy_map (np.ndarray, optional): Entropy map, shape [H,W].\n",
    "            iqr_map (np.ndarray, optional): IQR map, shape [H,W].\n",
    "            range95_map (np.ndarray, optional): 95% range map, shape [H,W].\n",
    "            samples (np.ndarray, optional): Raw probability samples, shape [N,H,W].\n",
    "    \"\"\"\n",
    "    # Ensure dropout is active\n",
    "    model.train()\n",
    "    \n",
    "    # Collect Monte Carlo samples\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            logits = model(input_image)                 # [B, C, H, W] or [C, H, W]\n",
    "            prob = torch.sigmoid(logits)                # convert logits to probability\n",
    "            probs_list.append(prob.cpu().numpy())\n",
    "    \n",
    "    # Stack to shape [N, B, C, H, W] or [N, C, H, W]\n",
    "    probs = np.stack(probs_list, axis=0)\n",
    "    # Squeeze channel dim if C=1, and batch dim if B=1\n",
    "    if probs.ndim == 5 and probs.shape[2] == 1:\n",
    "        probs = probs.squeeze(2)\n",
    "    if probs.ndim == 4 and probs.shape[1] == 1:\n",
    "        probs = probs[:, 0, ...]\n",
    "\n",
    "    # Basic \n",
    "    mean_pred = np.mean(probs, axis=0)\n",
    "    var_map   = np.var(probs, axis=0) if num_samples > 1 else np.zeros_like(mean_pred)\n",
    "\n",
    "    outputs = [mean_pred, var_map]\n",
    "\n",
    "    # Standard deviation\n",
    "    if return_std:\n",
    "        std_map = np.sqrt(var_map)\n",
    "        outputs.append(std_map)\n",
    "\n",
    "    if return_entropy:\n",
    "        eps = 1e-12\n",
    "        p   = np.clip(mean_pred, eps, 1 - eps)   # vyhnúť sa log(0)\n",
    "        entropy_map = -(p * np.log(p) + (1 - p) * np.log(1 - p))\n",
    "        outputs.append(entropy_map)\n",
    "\n",
    "    # Inter-quartile range (IQR)\n",
    "    if return_iqr:\n",
    "        q75 = np.percentile(probs, 75, axis=0)\n",
    "        q25 = np.percentile(probs, 25, axis=0)\n",
    "        iqr_map = q75 - q25\n",
    "        outputs.append(iqr_map)\n",
    "\n",
    "    # 95% range (between 2.5th and 97.5th percentiles)\n",
    "    if return_range95:\n",
    "        upper = np.percentile(probs, 97.5, axis=0)\n",
    "        lower = np.percentile(probs, 2.5, axis=0)\n",
    "        range95_map = upper - lower\n",
    "        outputs.append(range95_map)\n",
    "\n",
    "    # Raw samples\n",
    "    if return_samples:\n",
    "        outputs.append(probs)\n",
    "\n",
    "    return tuple(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 – Demonstration on one example from the test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_uncertainty_maps(original,ground_truth, segmentation, uncertainties_dict):\n",
    "    num_maps = len(uncertainties_dict)\n",
    "    cols = max(2, num_maps)\n",
    "    fig, axes = plt.subplots(2, cols, figsize=(4 * cols, 8))\n",
    "\n",
    "    # First Row : RTG\n",
    "    ax = axes[0, 0]\n",
    "    img = np.transpose(original, (1, 2, 0)) if original.ndim == 3 else original\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title('RTG')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # First row: Ground Truth \n",
    "    ax = axes[0, 2]\n",
    "    ax.imshow(ground_truth.squeeze(), cmap='gray')\n",
    "    ax.set_title('Ground Truth')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # first row: Segmentation\n",
    "    ax = axes[0, 1]\n",
    "    ax.imshow(segmentation.squeeze(), cmap='gray')\n",
    "    ax.set_title('Segmentácia')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Empty fields in the first row, if num_maps < cols\n",
    "    for j in range(2, cols):\n",
    "        axes[0, j].axis('off')\n",
    "\n",
    "    # second row: Uncertainty maps\n",
    "    for j, (key, unc_map) in enumerate(uncertainties_dict.items()):\n",
    "        ax = axes[1, j]\n",
    "        data = unc_map.squeeze()\n",
    "\n",
    "        if key.lower() == 'entropia':\n",
    "            # entropy has range [0, log(2)]\n",
    "            vmin, vmax = 0.0, np.log(2.0)\n",
    "            im = ax.imshow(data, cmap='jet', vmin=vmin, vmax=vmax, alpha=0.8)\n",
    "        else:\n",
    "            # cut outliers for other maps\n",
    "            vmin = 0.0\n",
    "            vmax = np.percentile(data, 99)\n",
    "            im = ax.imshow(data, cmap='jet', vmin=vmin, vmax=vmax, alpha=0.8)\n",
    "\n",
    "        ax.set_title(key)\n",
    "        ax.axis('off')\n",
    "        fig.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "    for j in range(num_maps, cols):\n",
    "        axes[1, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) one sample from the test set\n",
    "sample_batch = next(iter(test_loader))\n",
    "inputs, labels = sample_batch  \n",
    "sample = inputs.to(device)[0].unsqueeze(0)\n",
    "label = labels[0]  \n",
    "\n",
    "# 2) MC-dropout (N=50 samples) \n",
    "mean_pred_np, var_map, std_map, entropy_map, iqr_map, range95_map = mc_dropout_predict(\n",
    "    model,\n",
    "    sample,               #  [B,C,H,W]\n",
    "    num_samples     = 50,\n",
    "    return_std      = True,\n",
    "    return_entropy  = True,\n",
    "    return_iqr      = True,\n",
    "    return_range95  = True,\n",
    "    return_samples  = False\n",
    ")\n",
    "\n",
    "# 3) preparation for visualization\n",
    "orig_np   = sample[0].cpu().numpy()            # (3,H,W)\n",
    "gt_np     = label[0].squeeze().cpu().numpy()   # (H,W)\n",
    "segmentation_np  = (mean_pred_np.squeeze() > PRED_THRESH).astype(float)     # (H,W)\n",
    "\n",
    "# 4) tupple of uncertainty maps\n",
    "uncertainties = {\n",
    "    'Variance'            : var_map,\n",
    "    'Std dev'             : std_map,\n",
    "    'Entropy'             : entropy_map,\n",
    "    'IQR'                 : iqr_map,\n",
    "    'Range 95%'           : range95_map\n",
    "}\n",
    "\n",
    "# 5) Visualization – first row: orig | GT | pred ; second: uncertainty maps\n",
    "visualize_multiple_uncertainty_maps(orig_np,gt_np, segmentation_np, uncertainties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: visualization of 5 random samples from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1) reproduceable set of data \n",
    "random.seed(2025)\n",
    "rand_idx = random.sample(range(len(test_dataset)), 5)   \n",
    "\n",
    "# 2) for through the random samples\n",
    "for k, idx in enumerate(rand_idx, 1):\n",
    "    img, gt = test_dataset[idx]          # tensors (C,H,W)  &  (1,H,W)\n",
    "\n",
    "    img_batch = img.unsqueeze(0).to(device)  # (1,C,H,W) \n",
    "\n",
    "    # MC-dropout (N = 50 samples)\n",
    "    mean_np, var_map, std_map, ent_map, iqr_map, r95_map = mc_dropout_predict(\n",
    "        model,\n",
    "        img_batch,\n",
    "        num_samples     = 50,\n",
    "        return_std      = True,\n",
    "        return_entropy  = True,\n",
    "        return_iqr      = True,\n",
    "        return_range95  = True,\n",
    "        return_samples  = False\n",
    "    )\n",
    "\n",
    "    # visualization preparation\n",
    "    orig_np  = img.cpu().numpy()                     # (3,H,W)\n",
    "    gt_np    = gt.squeeze().cpu().numpy()            # (H,W)\n",
    "    pred_bin = (mean_np.squeeze() > PRED_THRESH).astype(float)\n",
    "\n",
    "    uncert = {\n",
    "        'Variance'  : var_map,\n",
    "        'Std dev'   : std_map,\n",
    "        'Entropy'   : ent_map,\n",
    "        'IQR'       : iqr_map,\n",
    "        'Range 95%' : r95_map\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Príklad {k}  (dataset index {idx}) ===\")\n",
    "    visualize_multiple_uncertainty_maps(orig_np, gt_np, pred_bin, uncert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_uncertainty_maps(original, ground_truth, segmentation, uncertainties_dict):\n",
    "    num_maps = len(uncertainties_dict)\n",
    "    cols = max(2, num_maps)\n",
    "    fig, axes = plt.subplots(2, cols, figsize=(4 * cols, 8))\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    img = np.transpose(original, (1, 2, 0)) if original.ndim == 3 else original\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title('RTG')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = axes[0, 2]\n",
    "    ax.imshow(ground_truth.squeeze(), cmap='gray')\n",
    "    ax.set_title('Ground Truth')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.imshow(segmentation.squeeze(), cmap='gray')\n",
    "    ax.set_title('Segmentácia')\n",
    "    ax.axis('off')\n",
    "\n",
    "    for j in range(2, cols):\n",
    "        axes[0, j].axis('off')\n",
    "\n",
    "    normalized_uncertainties = {}\n",
    "    for key, unc_map in uncertainties_dict.items():\n",
    "        data = unc_map.squeeze()\n",
    "        \n",
    "        if key.lower() == 'entropy':\n",
    "            normalized = data / np.log(2.0)\n",
    "        else:\n",
    "            max_val = np.percentile(data, 99)\n",
    "            normalized = data / max_val\n",
    "            normalized = np.clip(normalized, 0, 1)\n",
    "        \n",
    "        normalized_uncertainties[key] = normalized\n",
    "\n",
    "    for j, (key, unc_map) in enumerate(normalized_uncertainties.items()):\n",
    "        ax = axes[1, j]\n",
    "        im = ax.imshow(unc_map.squeeze(), cmap='jet', vmin=0, vmax=1, alpha=0.8)\n",
    "        ax.set_title(f\"{key} (norm.)\")\n",
    "        ax.axis('off')\n",
    "        fig.colorbar(im, ax=ax, shrink=0.6)\n",
    "\n",
    "    for j in range(num_maps, cols):\n",
    "        axes[1, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(2025)\n",
    "rand_idx = random.sample(range(len(test_dataset)), 5)   \n",
    "\n",
    "for k, idx in enumerate(rand_idx, 1):\n",
    "    img, gt = test_dataset[idx]       \n",
    "\n",
    "    img_batch = img.unsqueeze(0).to(device)  \n",
    "\n",
    "    # MC-dropout (N = 50 samples)\n",
    "    mean_np, var_map, std_map, ent_map, iqr_map, r95_map = mc_dropout_predict(\n",
    "        model,\n",
    "        img_batch,\n",
    "        num_samples     = 50,\n",
    "        return_std      = True,\n",
    "        return_entropy  = True,\n",
    "        return_iqr      = True,\n",
    "        return_range95  = True,\n",
    "        return_samples  = False\n",
    "    )\n",
    "\n",
    "    orig_np  = img.cpu().numpy()                     # (3,H,W)\n",
    "    gt_np    = gt.squeeze().cpu().numpy()            # (H,W)\n",
    "    pred_bin = (mean_np.squeeze() > PRED_THRESH).astype(float)\n",
    "\n",
    "    uncert = {\n",
    "        'Variance'  : var_map,\n",
    "        'Std dev'   : std_map,\n",
    "        'Entropy'   : ent_map,\n",
    "        'IQR'       : iqr_map,\n",
    "        'Range 95%' : r95_map\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Príklad {k}  (dataset index {idx}) ===\")\n",
    "    visualize_multiple_uncertainty_maps(orig_np, gt_np, pred_bin, uncert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  time\n",
    "\n",
    "def visualize_entropy_vs_N(original, ground_truth, segmentation, entropies_dict):\n",
    "    \"\"\"\n",
    "    1. riadok: orig | GT | pred (N=50)\n",
    "    2. riadok: entropia pre N=10,20,30,40,50\n",
    "    \"\"\"\n",
    "    cols = max(3, len(entropies_dict))\n",
    "    fig, axes = plt.subplots(2, cols, figsize=(4*cols, 8))\n",
    "\n",
    "    # 1) orig | GT | pred\n",
    "    axes[0,0].imshow(np.transpose(original,(1,2,0)), cmap='gray')\n",
    "    axes[0,0].set_title(\"RTG\");         axes[0,0].axis('off')\n",
    "\n",
    "    axes[0,1].imshow(ground_truth, cmap='gray')\n",
    "    axes[0,1].set_title(\"GT maska\");    axes[0,1].axis('off')\n",
    "\n",
    "    axes[0,2].imshow(segmentation, cmap='gray')\n",
    "    axes[0,2].set_title(\"Predikcia (N=50)\"); axes[0,2].axis('off')\n",
    "\n",
    "    for j in range(3, cols):\n",
    "        axes[0,j].axis('off')\n",
    "\n",
    "    # 2) entropy for different N\n",
    "    for j,(name,emap) in enumerate(entropies_dict.items()):\n",
    "        ax = axes[1,j]\n",
    "        im = ax.imshow(emap.squeeze(), cmap='jet',\n",
    "                       vmin=0.0, vmax=np.log(2.0))      # fix scale 0-ln2\n",
    "        ax.set_title(name)\n",
    "        ax.axis('off')\n",
    "        fig.colorbar(im, ax=ax, fraction=0.045, pad=0.04)\n",
    "\n",
    "    for j in range(len(entropies_dict), cols):\n",
    "        axes[1,j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "N_LIST = [0,5,10,15,20,25,30,35,40,45,50]         \n",
    "\n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_means, dice_stds = [], []\n",
    "\n",
    "print(\"⇢ Výpočet Dice pre rôzne počty vzoriek N …\")\n",
    "for N in N_LIST:\n",
    "    dice_vals = []\n",
    "    t0 = time.time()\n",
    "    for img, gt in test_loader_1:\n",
    "        img = img.to(device)\n",
    "\n",
    "        # --- baseline N=0 (without dropout) \n",
    "        if N == 0:\n",
    "            model.eval()                                     \n",
    "            with torch.no_grad():\n",
    "                logits = model(img)\n",
    "                mean_np = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
    "        # --- MC-dropout N>0\n",
    "        else:\n",
    "            mean_np, *_ = mc_dropout_predict(\n",
    "                model, img,\n",
    "                num_samples     = N,\n",
    "                return_std      = False,\n",
    "                return_entropy  = False,\n",
    "                return_iqr      = False,\n",
    "                return_range95  = False,\n",
    "                return_samples  = False\n",
    "            )\n",
    "\n",
    "        pred_bin = torch.from_numpy(mean_np)[None, None] > PRED_THRESH   # (1,1,H,W)\n",
    "        d = dice_score(pred_bin.float(), gt.float())\n",
    "        dice_vals.append(d)\n",
    "\n",
    "    dice_means.append(np.mean(dice_vals))\n",
    "    dice_stds .append(np.std (dice_vals))\n",
    "    print(f\"  N={N:2d} | Dice: {dice_means[-1]:.4f} ± {dice_stds[-1]:.4f} \"\n",
    "          f\"| čas: {time.time()-t0:.1f}s\")\n",
    "\n",
    "y_min = min(dice_means) - 0.02\n",
    "y_max = max(dice_means) + 0.02\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.errorbar(N_LIST, dice_means, yerr=dice_stds,\n",
    "             marker='o', capsize=4, label=\"MC-dropout\")\n",
    "# baseline (N=0) horizontal line\n",
    "plt.axhline(dice_means[0], linestyle='--', color='green',\n",
    "            label=f\"Baseline N=0 (Dice {dice_means[0]:.3f})\")\n",
    "\n",
    "plt.xlabel(\"MC Samples count N\")\n",
    "plt.ylabel(\"Average Dice on test set\")\n",
    "plt.title(\"Stabilization Dice with increasing N\")\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(N_LIST)\n",
    "plt.grid(True, axis='y', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, gt = test_dataset[idx]\n",
    "img_batch = img.unsqueeze(0).to(device)\n",
    "\n",
    "mean_50, *_ = mc_dropout_predict(\n",
    "    model, img_batch, num_samples=50,\n",
    "    return_std=False, return_entropy=False,\n",
    "    return_iqr=False, return_range95=False\n",
    ")\n",
    "\n",
    "orig_np  = img.cpu().numpy()\n",
    "gt_np    = gt.squeeze().cpu().numpy()\n",
    "pred_np  = (mean_50.squeeze() > PRED_THRESH).astype(float)\n",
    "\n",
    "entropy_maps = {}\n",
    "for N in [10, 20, 30, 40, 50]:\n",
    "    _, _, ent_map = mc_dropout_predict(\n",
    "        model, img_batch, num_samples=N,\n",
    "        return_std=False, return_entropy=True,\n",
    "        return_iqr=False, return_range95=False\n",
    "    )\n",
    "    entropy_maps[f\"Entropy\\nN={N}\"] = ent_map\n",
    "\n",
    "def visualize_entropy_vs_N(original, ground_truth, segmentation, entropies_dict):\n",
    "    cols = max(3, len(entropies_dict))\n",
    "    fig, axes = plt.subplots(2, cols, figsize=(4*cols, 8))\n",
    "\n",
    "    axes[0,0].imshow(np.transpose(original,(1,2,0)), cmap='gray')\n",
    "    axes[0,0].set_title(\"RTG\");         axes[0,0].axis('off')\n",
    "\n",
    "    axes[0,1].imshow(ground_truth, cmap='gray')\n",
    "    axes[0,1].set_title(\"GT maska\");    axes[0,1].axis('off')\n",
    "\n",
    "    axes[0,2].imshow(segmentation, cmap='gray')\n",
    "    axes[0,2].set_title(\"Predikcia (N=50)\"); axes[0,2].axis('off')\n",
    "\n",
    "    for j in range(3, cols):\n",
    "        axes[0,j].axis('off')\n",
    "\n",
    "    for j,(name,emap) in enumerate(entropies_dict.items()):\n",
    "        ax = axes[1,j]\n",
    "        im = ax.imshow(emap.squeeze(), cmap='jet',\n",
    "                       vmin=0.0, vmax=np.log(2.0))\n",
    "        ax.set_title(name)\n",
    "        ax.axis('off')\n",
    "        fig.colorbar(im, ax=ax, fraction=0.045, pad=0.04)\n",
    "\n",
    "    for j in range(len(entropies_dict), cols):\n",
    "        axes[1,j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_entropy_vs_N(orig_np, gt_np, pred_np, entropy_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dropout_variability(model, input_image):\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        out1 = model(input_image)\n",
    "        out2 = model(input_image)\n",
    "    diff = torch.abs(out1 - out2).mean().item()\n",
    "    print(f\"Average difference between two outputs: {diff:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dropout_variability(model, img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ----------------------------- Dice metric -----------------------------\n",
    "def dice_score_without_threshold(pred_mask, true_mask):\n",
    "    \"\"\"\n",
    "    pred_mask, true_mask: torch.Tensor shape (1, 1, H, W), hodnoty {0,1} alebo float\n",
    "    \"\"\"\n",
    "    smooth = 1e-6\n",
    "    pred_flat = pred_mask.view(-1)\n",
    "    true_flat = true_mask.view(-1)\n",
    "    intersection = (pred_flat * true_flat).sum()\n",
    "    return (2. * intersection + smooth) / (pred_flat.sum() + true_flat.sum() + smooth)\n",
    "\n",
    "# --------------------------- DICE vs N GRAPF -----------------------------\n",
    "N_LIST = [0,5,10,15,20,25,30,35,40,45,50]         \n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_means, dice_stds = [], []\n",
    "print(\"⇢ Výpočet Dice pre rôzne počty vzoriek N …\")\n",
    "for N in N_LIST:\n",
    "    dice_vals = []\n",
    "    t0 = time.time()\n",
    "    for img, gt in test_loader_1:\n",
    "        img = img.to(device)\n",
    "\n",
    "        if N == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(img)\n",
    "                mean_np = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
    "        else:\n",
    "            mean_np, *_ = mc_dropout_predict(\n",
    "                model, img,\n",
    "                num_samples=N,\n",
    "                return_entropy=False\n",
    "            )\n",
    "\n",
    "        pred_bin = torch.from_numpy(mean_np)[None, None] > PRED_THRESH\n",
    "        d = dice_score_without_threshold(pred_bin.float(), gt.float())\n",
    "        dice_vals.append(d)\n",
    "\n",
    "    dice_means.append(np.mean(dice_vals))\n",
    "    dice_stds .append(np.std (dice_vals))\n",
    "    print(f\"  N={N:2d} | Dice: {dice_means[-1]:.4f} ± {dice_stds[-1]:.4f} \"\n",
    "          f\"| čas: {time.time()-t0:.1f}s\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.errorbar(N_LIST, dice_means, yerr=dice_stds,\n",
    "             marker='o', capsize=4, label=\"MC-dropout\")\n",
    "plt.axhline(dice_means[0], linestyle='--', color='green',\n",
    "            label=f\"Baseline N=0 (Dice {dice_means[0]:.3f})\")\n",
    "plt.xlabel(\"MC vzorky N\")\n",
    "plt.ylabel(\"Priemerný Dice (test sada)\")\n",
    "plt.title(\"Dice stabilita s rastúcim N\")\n",
    "plt.ylim(min(dice_means)-0.02, max(dice_means)+0.02)\n",
    "plt.xticks(N_LIST)\n",
    "plt.grid(True, axis='y', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------- VIZUALIZATION of entropy ------------------------\n",
    "random.seed(42)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, gt = test_dataset[idx]\n",
    "img_batch = img.unsqueeze(0).to(device)\n",
    "\n",
    "mean_50, *_ = mc_dropout_predict(\n",
    "    model, img_batch, num_samples=50, return_entropy=False\n",
    ")\n",
    "orig_np  = img.cpu().numpy()\n",
    "gt_np    = gt.squeeze().cpu().numpy()\n",
    "pred_np  = (mean_50.squeeze() > PRED_THRESH).astype(float)\n",
    "\n",
    "entropy_maps = {}\n",
    "for N in [10, 20, 30, 40, 50]:\n",
    "    _, _, ent_map = mc_dropout_predict(\n",
    "        model, img_batch, num_samples=N,\n",
    "        return_entropy=True\n",
    "    )\n",
    "    entropy_maps[f\"Entropy\\nN={N}\"] = ent_map\n",
    "\n",
    "def visualize_entropy_vs_N(original, ground_truth, segmentation, entropies_dict):\n",
    "    cols = max(3, len(entropies_dict))\n",
    "    fig, axes = plt.subplots(2, cols, figsize=(4*cols, 8))\n",
    "    axes[0,0].imshow(np.transpose(original,(1,2,0)), cmap='gray')\n",
    "    axes[0,0].set_title(\"RTG\");         axes[0,0].axis('off')\n",
    "    axes[0,1].imshow(ground_truth, cmap='gray')\n",
    "    axes[0,1].set_title(\"GT maska\");    axes[0,1].axis('off')\n",
    "    axes[0,2].imshow(segmentation, cmap='gray')\n",
    "    axes[0,2].set_title(\"Predikcia (N=50)\"); axes[0,2].axis('off')\n",
    "    for j in range(3, cols): axes[0,j].axis('off')\n",
    "    for j,(name,emap) in enumerate(entropies_dict.items()):\n",
    "        ax = axes[1,j]\n",
    "        im = ax.imshow(emap.squeeze(), cmap='jet', vmin=0.0, vmax=np.log(2.0))\n",
    "        ax.set_title(name)\n",
    "        ax.axis('off')\n",
    "        fig.colorbar(im, ax=ax, fraction=0.045, pad=0.04)\n",
    "    for j in range(len(entropies_dict), cols):\n",
    "        axes[1,j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_entropy_vs_N(orig_np, gt_np, pred_np, entropy_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice(pred_mask, true_mask):\n",
    "    pred_flat = pred_mask.flatten()\n",
    "    true_flat = true_mask.flatten()\n",
    "    return f1_score(true_flat, pred_flat)\n",
    "\n",
    "threshold = PRED_THRESH if 'PRED_THRESH' in globals() else 0.5\n",
    "sample_counts = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "for N in sample_counts:\n",
    "    dice_scores = []\n",
    "    start = time.time()\n",
    "    for image, mask in val_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.squeeze().cpu().numpy().astype(bool)\n",
    "\n",
    "        if N == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "                prob = torch.sigmoid(output).squeeze().cpu().numpy()\n",
    "        else:\n",
    "            mean_pred, *_ = mc_dropout_predict(model, image, num_samples=N)\n",
    "            prob = mean_pred\n",
    "\n",
    "        pred_bin = prob > threshold\n",
    "        dice = compute_dice(pred_bin, mask)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    std_dice = np.std(dice_scores)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  N={N:2d} | Dice: {mean_dice:.4f} ± {std_dice:.4f} | čas: {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Overlap histogram of entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import gaussian_kde   \n",
    "\n",
    "N_OPT = 30                       \n",
    "BINS  = 60                        \n",
    "\n",
    "# 1) loader s batch_size=1\n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "correct_vals  = []  \n",
    "incorrect_vals = []   \n",
    "\n",
    "print(f\"⇢ Zbieram std pre všetky pixely test-setu (N={N_OPT}) …\")\n",
    "for img, gt in test_loader_1:\n",
    "    img = img.to(device)                           # (1,3,H,W)\n",
    "\n",
    "    # MC-dropout → entropy\n",
    "    _, _,std_map= mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples     = N_OPT,\n",
    "        return_std      = True,\n",
    "        return_entropy  = False,\n",
    "        return_iqr      = False,\n",
    "        return_range95  = False,\n",
    "        return_samples  = False\n",
    "    )\n",
    "    std_map = std_map.squeeze()                    # (H,W)\n",
    "\n",
    "    # bináry pred\n",
    "    mean_np, *_ = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples     = N_OPT,\n",
    "        return_std      = False,\n",
    "        return_entropy  = False,\n",
    "        return_iqr      = False,\n",
    "        return_range95  = False,\n",
    "        return_samples  = False\n",
    "    )\n",
    "    pred_bin = (mean_np.squeeze() > PRED_THRESH).astype(np.uint8)   # (H,W)\n",
    "    gt_bin   = gt.squeeze().numpy().astype(np.uint8)\n",
    "\n",
    "    # masks\n",
    "    correct_mask   = (pred_bin == gt_bin)\n",
    "    incorrect_mask = ~correct_mask\n",
    "\n",
    "    correct_vals  .append(std_map[correct_mask])\n",
    "    incorrect_vals.append(std_map[incorrect_mask])\n",
    "\n",
    "# to 1D\n",
    "correct_vals   = np.concatenate(correct_vals)\n",
    "incorrect_vals = np.concatenate(incorrect_vals)\n",
    "\n",
    "print(f\"  • number of correct pixels   : {correct_vals.size:,}\")\n",
    "print(f\"  • number of incorrect pixels : {incorrect_vals.size:,}\")\n",
    "\n",
    "# 2) Histogram\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "plt.hist(correct_vals,   bins=BINS, density=True, alpha=0.5,\n",
    "         label='Correct',   color='tab:blue')\n",
    "plt.hist(incorrect_vals, bins=BINS, density=True, alpha=0.5,\n",
    "         label='Incorrect', color='tab:orange')\n",
    "\n",
    "for vals, col in [(correct_vals,'tab:blue'), (incorrect_vals,'tab:orange')]:\n",
    "    kde = gaussian_kde(vals)\n",
    "    xs  = np.linspace(0, np.log(2), 300)\n",
    "    plt.plot(xs, kde(xs), color=col)\n",
    "\n",
    "plt.xlabel(\"Pixel-wise std  $H$  (bits)\")\n",
    "plt.title(f\"Overlap std – Correct vs Incorrect  (N={N_OPT})\")\n",
    "plt.legend()\n",
    "plt.xlim(0, np.log(2))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Kvantifikované prekrývanie (area of overlap)\n",
    "hist_c , edges = np.histogram(correct_vals  , bins=BINS, density=True)\n",
    "hist_ic, _     = np.histogram(incorrect_vals, bins=edges, density=True)\n",
    "overlap = np.sum(np.minimum(hist_c, hist_ic) * np.diff(edges))\n",
    "print(f\"Area of overlap = {overlap:.3f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OPT = 30\n",
    "BINS  = 60\n",
    "metrics = [\"Variance\", \"Std\", \"Entropy\", \"IQR\", \"Range95\"]\n",
    "\n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "corr_vals, inc_vals = {m: [] for m in metrics}, {m: [] for m in metrics}\n",
    "\n",
    "print(f\"⇢ Zbieram metriky (entropia z mean-p, N={N_OPT}) …\")\n",
    "for img, gt in test_loader_1:\n",
    "    img = img.to(device)\n",
    "\n",
    "    mean_np, var_map, std_map,  iqr_map, r95_map = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples     = N_OPT,\n",
    "        return_std      = True,\n",
    "        return_entropy  = False,  \n",
    "        return_iqr      = True,\n",
    "        return_range95  = True,\n",
    "        return_samples  = False\n",
    "    )\n",
    "    # ── entropy ────────────────────────────\n",
    "    p_mean = np.clip(mean_np.squeeze(), 1e-6, 1-1e-6)      # (H,W)\n",
    "    ent_map = -(p_mean*np.log(p_mean) + (1-p_mean)*np.log(1-p_mean))\n",
    "\n",
    "    pred_bin = (p_mean > PRED_THRESH).astype(np.uint8)\n",
    "    gt_bin   = gt.squeeze().numpy().astype(np.uint8)\n",
    "\n",
    "    corr_mask = (pred_bin == gt_bin)\n",
    "    inc_mask  = ~corr_mask\n",
    "\n",
    "    maps = {\n",
    "        \"Variance\": var_map,\n",
    "        \"Std\":      std_map,\n",
    "        \"Entropy\":  ent_map,\n",
    "        \"IQR\":      iqr_map,\n",
    "        \"Range95\":  r95_map\n",
    "    }\n",
    "    for m in metrics:\n",
    "        corr_vals[m].append(maps[m][corr_mask])\n",
    "        inc_vals [m].append(maps[m][inc_mask])\n",
    "\n",
    "for m in metrics:\n",
    "    corr_vals[m] = np.concatenate(corr_vals[m])\n",
    "    inc_vals [m] = np.concatenate(inc_vals[m])\n",
    "\n",
    "# ── visualization & overlap ───────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8)); axes = axes.flatten()\n",
    "overlap_num = {}\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    corr, inc = corr_vals[m], inc_vals[m]\n",
    "\n",
    "    vmax = np.percentile(np.concatenate([corr, inc]), 99)\n",
    "    bins = np.linspace(0, vmax, BINS+1)\n",
    "\n",
    "    h_c, _ = np.histogram(corr, bins=bins, density=True)\n",
    "    h_i, _ = np.histogram(inc , bins=bins, density=True)\n",
    "    ax.hist(corr, bins=bins, alpha=0.5, density=True, label='Correct')\n",
    "    ax.hist(inc , bins=bins, alpha=0.5, density=True, label='Incorrect')\n",
    "\n",
    "    overlap = np.sum(np.minimum(h_c, h_i) * np.diff(bins))\n",
    "    overlap_num[m] = overlap\n",
    "\n",
    "    ax.set_title(f\"{m}  (overlap={overlap:.2f})\")\n",
    "    ax.set_xlabel(m); ax.set_ylabel(\"Density\"); ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "axes[-1].axis('off')\n",
    "fig.suptitle(f\"Pixel-wise overlap – MC-dropout N={N_OPT}\", fontsize=16)\n",
    "fig.tight_layout(rect=[0,0,1,0.95])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nArea of overlap :\")\n",
    "for m in metrics:\n",
    "    print(f\"  {m:8s}: {overlap_num[m]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 : Reliability heat-map\n",
    "###     error-rate vs. epistemic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "N_OPT = NUM_SAMPLES          \n",
    "K_BINS = 20        \n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "u_vals, err_vals = [], []\n",
    "\n",
    "print(f\"Going through test-set, N={N_OPT}, K={K_BINS} …\")\n",
    "for img, gt in test_loader_1:\n",
    "    img = img.to(device)                     # (1,3,H,W)\n",
    "\n",
    "    # MC-dropout → mean + entropy\n",
    "    mean_np, _, ent_map= mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples     = N_OPT,\n",
    "        return_entropy  = True\n",
    "    )\n",
    "    ent_map = ent_map.squeeze()              # (H,W)\n",
    "\n",
    "    # binary pred a error mASK\n",
    "    pred_bin = (mean_np.squeeze() > PRED_THRESH).astype(np.uint8)\n",
    "    gt_bin   = gt.squeeze().numpy().astype(np.uint8)\n",
    "    err_mask = (pred_bin != gt_bin).astype(np.uint8)\n",
    "\n",
    "    bins = np.linspace(0.0, np.log(2.0), K_BINS + 1)        # 0 … ln2\n",
    "    bin_idx = np.digitize(ent_map, bins) - 1                # 0 … K-1\n",
    "\n",
    "    for k in range(K_BINS):\n",
    "        in_bin = (bin_idx == k)\n",
    "        if in_bin.sum() == 0:\n",
    "            continue\n",
    "        err_rate = err_mask[in_bin].mean()                 \n",
    "        u_center = 0.5 * (bins[k] + bins[k+1])             \n",
    "        u_vals.append(u_center)\n",
    "        err_vals.append(err_rate)\n",
    "\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(\n",
    "    u_vals, err_vals,\n",
    "    bins=[K_BINS, K_BINS],        \n",
    "    range=[[0, np.log(2.0)], [0, 1]]\n",
    ")\n",
    "\n",
    "mean_err_per_bin = []\n",
    "for i in range(K_BINS):\n",
    "    mask = (np.array(u_vals) >= xedges[i]) & (np.array(u_vals) < xedges[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        mean_err_per_bin.append(err_vals*np.array(mask))\n",
    "    else:\n",
    "        mean_err_per_bin.append([])\n",
    "\n",
    "mean_err = [\n",
    "    np.mean(np.array(err_vals)[(np.array(u_vals) >= xedges[i]) &\n",
    "                               (np.array(u_vals) <  xedges[i+1])])\n",
    "    if ((np.array(u_vals) >= xedges[i]) & (np.array(u_vals) < xedges[i+1])).any()\n",
    "    else np.nan\n",
    "    for i in range(K_BINS)\n",
    "]\n",
    "\n",
    "# Spearman correlation\n",
    "rho, p_val = spearmanr(u_vals, err_vals)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "# heat-map\n",
    "plt.imshow(\n",
    "    H.T, origin='lower',\n",
    "    extent=[0, np.log(2.0), 0, 1], aspect='auto',\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(label = 'Number of (slice, bin) points')\n",
    "# red average line\n",
    "centers = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "plt.plot(centers, mean_err, color='red', linewidth=2, label='Average error-rate')\n",
    "\n",
    "plt.xlabel('Epistemic uncertainty  $H(\\\\hat p)$')\n",
    "plt.ylabel('Error-rate')\n",
    "plt.title(f'Reliability heat-map  –  N={N_OPT}  |  Spearman ρ={rho:.2f} (p={p_val:.2e})')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Scatter VVC vs (1 – Dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import linregress, spearmanr\n",
    "\n",
    "N_OPT = NUM_SAMPLES          # sweet-spot \n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_list, vvc_list = [], []\n",
    "\n",
    "print(\"⇢ Počítam VVC a 1-Dice pre každú snímku test-setu …\")\n",
    "for idx, (img, gt) in enumerate(test_loader_1):\n",
    "    img = img.to(device)\n",
    "\n",
    "    mean_np, _,  samples = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples    = N_OPT,\n",
    "        return_std     = False,\n",
    "        return_entropy = False,\n",
    "        return_iqr     = False,\n",
    "        return_range95 = False,\n",
    "        return_samples = True         \n",
    "    )\n",
    "\n",
    "    # --- Dice for average mask -----------------------------------------\n",
    "    pred_bin = (mean_np > PRED_THRESH).astype(np.uint8)\n",
    "    d = dice_score(torch.from_numpy(pred_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5)          # tu 0.5 \n",
    "    dice_list.append(1 - d)                           \n",
    "\n",
    "    # --- VVC -------------------------------------------------------------\n",
    "    vols = samples.reshape(N_OPT, -1).sum(axis=1)    \n",
    "    muV  = vols.mean()\n",
    "    sigmaV = vols.std()\n",
    "    vvc_list.append(sigmaV / (muV + 1e-8))           \n",
    "\n",
    "# Scatter + linear regression \n",
    "x = np.array(dice_list)\n",
    "y = np.array(vvc_list)\n",
    "\n",
    "slope, intercept, r_val, p_val, _ = linregress(x, y)\n",
    "rho, p_spear = spearmanr(x, y)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(x, y, color=\"#c1440e\", s=30)\n",
    "plt.plot(x, slope*x + intercept, color=\"#c1440e\")\n",
    "plt.xlabel(\"1 – Dice\")\n",
    "plt.ylabel(\"VVC\")\n",
    "plt.title(f\"TTR (MC-dropout)   R²={r_val**2:.3f}   ρ={rho:.3f}\")\n",
    "plt.xlim(0, x.max()*1.05)\n",
    "plt.ylim(0, y.max()*1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLinear regression:  y = {slope:.3f}·x + {intercept:.4f}   \"\n",
    "      f\"(R² = {r_val**2:.3f},  p = {p_val:.2e})\")\n",
    "print(f\"Spearman ρ = {rho:.3f}  (p = {p_spear:.2e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_OPT = 80          # sweet-spot \n",
    "test_loader_1 = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_list, vvc_list = [], []\n",
    "\n",
    "print(\"⇢ Počítam VVC a 1-Dice pre každú snímku test-setu …\")\n",
    "for idx, (img, gt) in enumerate(test_loader_1):\n",
    "    img = img.to(device)\n",
    "\n",
    "    mean_np, _,  samples = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples    = N_OPT,\n",
    "        return_std     = False,\n",
    "        return_entropy = False,\n",
    "        return_iqr     = False,\n",
    "        return_range95 = False,\n",
    "        return_samples = True          \n",
    "    )\n",
    "\n",
    "    pred_bin = (mean_np > PRED_THRESH).astype(np.uint8)\n",
    "    d = dice_score(torch.from_numpy(pred_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5)          \n",
    "    dice_list.append(1 - d)                          \n",
    "\n",
    "    # --- VVC -------------------------------------------------------------\n",
    "    vols = samples.reshape(N_OPT, -1).sum(axis=1)     \n",
    "    muV  = vols.mean()\n",
    "    sigmaV = vols.std()\n",
    "    vvc_list.append(sigmaV / (muV + 1e-8))            \n",
    "\n",
    "x = np.array(dice_list)\n",
    "y = np.array(vvc_list)\n",
    "\n",
    "slope, intercept, r_val, p_val, _ = linregress(x, y)\n",
    "rho, p_spear = spearmanr(x, y)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(x, y, color=\"#c1440e\", s=30)\n",
    "plt.plot(x, slope*x + intercept, color=\"#c1440e\")\n",
    "plt.xlabel(\"1 – Dice\")\n",
    "plt.ylabel(\"VVC\")\n",
    "plt.title(f\"TTR (MC-dropout)   R²={r_val**2:.3f}   ρ={rho:.3f}\")\n",
    "plt.xlim(0, x.max()*1.05)\n",
    "plt.ylim(0, y.max()*1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLineárna regresia:  y = {slope:.3f}·x + {intercept:.4f}   \"\n",
    "      f\"(R² = {r_val**2:.3f},  p = {p_val:.2e})\")\n",
    "print(f\"Spearman ρ = {rho:.3f}  (p = {p_spear:.2e})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Mean vs Median aggregation (MC-dropout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES   = NUM_SAMPLES          # number of MC-dropout samples\n",
    "loader      = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_mean, dice_median = [], []\n",
    "\n",
    "print(f\"→ Evaluating Dice for MEAN vs MEDIAN (N={N_SAMPLES})\")\n",
    "for img, gt in loader:\n",
    "    img = img.to(device)\n",
    "\n",
    "    # run MC-dropout and keep the raw samples\n",
    "    mean_pred, _,  samples = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples    = N_SAMPLES,\n",
    "        return_entropy = False,\n",
    "        return_samples = True\n",
    "    )\n",
    "    samples = samples                                  # [N,H,W]\n",
    "    median_pred = np.median(samples, axis=0)\n",
    "\n",
    "    # binarise\n",
    "    mean_bin   = (mean_pred   > PRED_THRESH).astype(np.uint8)\n",
    "    median_bin = (median_pred > PRED_THRESH).astype(np.uint8)\n",
    "\n",
    "    # Dice\n",
    "    dice_mean.append(\n",
    "        dice_score(torch.from_numpy(mean_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5) )\n",
    "    dice_median.append(\n",
    "        dice_score(torch.from_numpy(median_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5) )\n",
    "\n",
    "dice_mean  = np.array(dice_mean)\n",
    "dice_median= np.array(dice_median)\n",
    "\n",
    "print(f\"Mean aggregation  :  Dice = {dice_mean.mean():.4f} ± {dice_mean.std():.4f}\")\n",
    "print(f\"Median aggregation:  Dice = {dice_median.mean():.4f} ± {dice_median.std():.4f}\")\n",
    "\n",
    "# --- bar-plot ------------------------------------------------------------\n",
    "labels = [\"Mean\", \"Median\"]\n",
    "vals   = [dice_mean.mean(), dice_median.mean()]\n",
    "errs   = [dice_mean.std(),  dice_median.std()]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.bar(labels, vals, yerr=errs, capsize=6, color=[\"steelblue\",\"orange\"])\n",
    "plt.ylabel(\"Average Dice (test set)\")\n",
    "plt.title(f\"Mean vs Median aggregation  |  N={N_SAMPLES}\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES   = NUM_SAMPLES          # number of MC-dropout samples\n",
    "loader      = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_mean, dice_median = [], []\n",
    "\n",
    "print(f\"→ Evaluating Dice for MEAN vs MEDIAN (N={N_SAMPLES})\")\n",
    "for img, gt in loader:\n",
    "    img = img.to(device)\n",
    "\n",
    "    # run MC-dropout and keep the raw samples\n",
    "    mean_pred, _,  samples = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples    = N_SAMPLES,\n",
    "        return_entropy = False,\n",
    "        return_samples = True\n",
    "    )\n",
    "    samples = samples                                  # [N,H,W]\n",
    "    median_pred = np.median(samples, axis=0)\n",
    "\n",
    "    # binarise\n",
    "    mean_bin   = (mean_pred   > PRED_THRESH).astype(np.uint8)\n",
    "    median_bin = (median_pred > PRED_THRESH).astype(np.uint8)\n",
    "\n",
    "    # Dice\n",
    "    dice_mean.append(\n",
    "        dice_score(torch.from_numpy(mean_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5) )\n",
    "    dice_median.append(\n",
    "        dice_score(torch.from_numpy(median_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5) )\n",
    "\n",
    "dice_mean  = np.array(dice_mean)\n",
    "dice_median= np.array(dice_median)\n",
    "\n",
    "print(f\"Mean aggregation  :  Dice = {dice_mean.mean():.4f} ± {dice_mean.std():.4f}\")\n",
    "print(f\"Median aggregation:  Dice = {dice_median.mean():.4f} ± {dice_median.std():.4f}\")\n",
    "\n",
    "# --- bar-plot ------------------------------------------------------------\n",
    "labels = [\"Mean\", \"Median\"]\n",
    "vals   = [dice_mean.mean(), dice_median.mean()]\n",
    "errs   = [dice_mean.std(),  dice_median.std()]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.bar(labels, vals, yerr=errs, capsize=6, color=[\"steelblue\",\"orange\"])\n",
    "plt.ylabel(\"Average Dice (test set)\")\n",
    "plt.title(f\"Mean vs Median aggregation  |  N={N_SAMPLES}\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any test slice\n",
    "random.seed(123)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, gt = test_dataset[idx]\n",
    "img_b   = img.unsqueeze(0).to(device)          # (1,3,H,W)\n",
    "\n",
    "# run MC-dropout and keep raw samples\n",
    "mean_pred, _,  samples = mc_dropout_predict(\n",
    "    model, img_b,\n",
    "    num_samples    = N_SAMPLES,    # keep same N as above (40)\n",
    "    return_entropy = False,\n",
    "    return_samples = True\n",
    ")\n",
    "median_pred = np.median(samples, axis=0)       # (H,W)\n",
    "\n",
    "# entropy from mean\n",
    "eps = 1e-12\n",
    "p_mean = np.clip(mean_pred,   eps, 1-eps)\n",
    "p_med  = np.clip(median_pred, eps, 1-eps)\n",
    "entropy_mean = -(p_mean*np.log(p_mean) + (1-p_mean)*np.log(1-p_mean))\n",
    "entropy_med  = -(p_med *np.log(p_med ) + (1-p_med )*np.log(1-p_med ))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n",
    "\n",
    "axes[0,0].imshow(np.transpose(img.cpu().numpy(), (1,2,0)), cmap='gray')\n",
    "axes[0,0].set_title(\"Original\");           axes[0,0].axis('off')\n",
    "\n",
    "axes[0,1].imshow((mean_pred.squeeze() > PRED_THRESH), cmap='gray')\n",
    "axes[0,1].set_title(f\"Prediction (mean, N={N_SAMPLES})\"); axes[0,2].axis('off')\n",
    "\n",
    "im1 = axes[1,0].imshow(entropy_mean, cmap='jet',\n",
    "                       vmin=0.0, vmax=np.log(2.0))\n",
    "axes[1,0].set_title(\"Entropy (mean)\");  axes[1,0].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1,0], fraction=0.05, pad=0.04)\n",
    "\n",
    "im2 = axes[1,1].imshow(entropy_med, cmap='jet',\n",
    "                       vmin=0.0, vmax=np.log(2.0))\n",
    "axes[1,1].set_title(\"Entropy (median)\"); axes[1,1].axis('off')\n",
    "plt.colorbar(im2, ax=axes[1,1], fraction=0.05, pad=0.04)\n",
    "\n",
    "diff = entropy_med - entropy_mean\n",
    "im3 = axes[1,2].imshow(diff, cmap='bwr',\n",
    "                       vmin=-0.2, vmax=0.2)   # symmetric range\n",
    "axes[1,2].set_title(\"Difference  (median – mean)\")\n",
    "axes[1,2].axis('off')\n",
    "plt.colorbar(im3, ax=axes[1,2], fraction=0.05, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_SAMPLES = NUM_SAMPLES      # MC-dropout samples per run\n",
    "K_RUNS    = 10      # how many independent runs\n",
    "THR       = PRED_THRESH\n",
    "\n",
    "loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_mean_runs   = []   # shape [K_RUNS]\n",
    "dice_median_runs = []\n",
    "\n",
    "for run in range(K_RUNS):\n",
    "    torch.manual_seed(run)\n",
    "    np.random.seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "    dice_mean_all, dice_median_all = [], []\n",
    "\n",
    "    for img, gt in loader:\n",
    "        img = img.to(device)\n",
    "\n",
    "        mean_pred, _, samples = mc_dropout_predict(\n",
    "            model, img,\n",
    "            num_samples    = N_SAMPLES,\n",
    "            return_samples = True\n",
    "        )\n",
    "        median_pred = np.median(samples, axis=0)\n",
    "\n",
    "        mean_bin   = (mean_pred   > THR).astype(np.uint8)\n",
    "        median_bin = (median_pred > THR).astype(np.uint8)\n",
    "\n",
    "        dice_mean_all.append(\n",
    "            dice_score(torch.from_numpy(mean_bin)[None,None].float(),\n",
    "                       gt.float(), threshold=0.5) )\n",
    "        dice_median_all.append(\n",
    "            dice_score(torch.from_numpy(median_bin)[None,None].float(),\n",
    "                       gt.float(), threshold=0.5) )\n",
    "\n",
    "    dice_mean_runs .append(np.mean(dice_mean_all))\n",
    "    dice_median_runs.append(np.mean(dice_median_all))\n",
    "\n",
    "dice_mean_runs   = np.array(dice_mean_runs)\n",
    "dice_median_runs = np.array(dice_median_runs)\n",
    "\n",
    "# --- paired t-test -------------------------------------------------------\n",
    "t_stat, p_val = ttest_rel(dice_mean_runs, dice_median_runs)\n",
    "\n",
    "print(f\"Mean   Dice: {dice_mean_runs.mean():.4f} ± {dice_mean_runs.std():.4f}\")\n",
    "print(f\"Median Dice: {dice_median_runs.mean():.4f} ± {dice_median_runs.std():.4f}\")\n",
    "print(f\"Paired t-test:  t = {t_stat:.3f}   p = {p_val:.4f}\")\n",
    "\n",
    "# --- box-plot ------------------------------------------------------------\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.boxplot([dice_mean_runs, dice_median_runs],\n",
    "            labels=[\"Mean\", \"Median\"], showmeans=True)\n",
    "plt.ylabel(\"Average Dice per run\")\n",
    "plt.title(f\"Mean vs Median  |  N={N_SAMPLES},  K={K_RUNS} runs\")\n",
    "plt.grid(axis=\"y\", alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_SAMPLES = NUM_SAMPLES      # MC-dropout samples per run\n",
    "K_RUNS    = 60      # how many independent runs\n",
    "THR       = PRED_THRESH\n",
    "\n",
    "loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_mean_runs   = []   # shape [K_RUNS]\n",
    "dice_median_runs = []\n",
    "\n",
    "for run in range(K_RUNS):\n",
    "    torch.manual_seed(run)\n",
    "    np.random.seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "    dice_mean_all, dice_median_all = [], []\n",
    "\n",
    "    for img, gt in loader:\n",
    "        img = img.to(device)\n",
    "\n",
    "        mean_pred, _, samples = mc_dropout_predict(\n",
    "            model, img,\n",
    "            num_samples    = N_SAMPLES,\n",
    "            return_samples = True\n",
    "        )\n",
    "        median_pred = np.median(samples, axis=0)\n",
    "\n",
    "        mean_bin   = (mean_pred   > THR).astype(np.uint8)\n",
    "        median_bin = (median_pred > THR).astype(np.uint8)\n",
    "\n",
    "        dice_mean_all.append(\n",
    "            dice_score(torch.from_numpy(mean_bin)[None,None].float(),\n",
    "                       gt.float(), threshold=0.5) )\n",
    "        dice_median_all.append(\n",
    "            dice_score(torch.from_numpy(median_bin)[None,None].float(),\n",
    "                       gt.float(), threshold=0.5) )\n",
    "\n",
    "    dice_mean_runs .append(np.mean(dice_mean_all))\n",
    "    dice_median_runs.append(np.mean(dice_median_all))\n",
    "\n",
    "dice_mean_runs   = np.array(dice_mean_runs)\n",
    "dice_median_runs = np.array(dice_median_runs)\n",
    "\n",
    "# --- paired t-test -------------------------------------------------------\n",
    "t_stat, p_val = ttest_rel(dice_mean_runs, dice_median_runs)\n",
    "\n",
    "print(f\"Mean   Dice: {dice_mean_runs.mean():.4f} ± {dice_mean_runs.std():.4f}\")\n",
    "print(f\"Median Dice: {dice_median_runs.mean():.4f} ± {dice_median_runs.std():.4f}\")\n",
    "print(f\"Paired t-test:  t = {t_stat:.3f}   p = {p_val:.4f}\")\n",
    "\n",
    "# --- box-plot ------------------------------------------------------------\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.boxplot([dice_mean_runs, dice_median_runs],\n",
    "            labels=[\"Mean\", \"Median\"], showmeans=True)\n",
    "plt.ylabel(\"Average Dice per run\")\n",
    "plt.title(f\"Mean vs Median  |  N={N_SAMPLES},  K={K_RUNS} runs\")\n",
    "plt.grid(axis=\"y\", alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 : Remove top-10 % most uncertain pixels and recompute Dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES   = NUM_SAMPLES      # MC-dropout samples for entropy\n",
    "TOP_FRAC    = 0.10    # fraction of pixels to remove\n",
    "THR         = PRED_THRESH\n",
    "loader      = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "dice_full, dice_masked = [], []\n",
    "\n",
    "def masked_dice(pred_bin, target_bin, valid_mask):\n",
    "    \"\"\"Dice only on pixels where valid_mask == 1.\"\"\"\n",
    "    pred   = pred_bin.float()   * valid_mask\n",
    "    target = target_bin.float() * valid_mask\n",
    "    inter  = (pred * target).sum()\n",
    "    union  = pred.sum() + target.sum()\n",
    "    return (2 * inter) / (union + 1e-7)\n",
    "\n",
    "print(f\"→ Evaluating Dice with top {int(TOP_FRAC*100)} % high-entropy pixels removed\")\n",
    "for img, gt in loader:\n",
    "    img = img.to(device)\n",
    "\n",
    "    # MC-dropout → entropy map\n",
    "    mean_pred, _,  ent_map, *_ = mc_dropout_predict(\n",
    "        model, img,\n",
    "        num_samples    = N_SAMPLES,\n",
    "        return_entropy = True\n",
    "    )\n",
    "    pred_bin = (mean_pred > THR).astype(np.uint8)\n",
    "    H, W     = ent_map.shape\n",
    "\n",
    "    # baseline Dice (all pixels)\n",
    "    dice_full.append(\n",
    "        dice_score(torch.from_numpy(pred_bin)[None,None].float(),\n",
    "                   gt.float(), threshold=0.5)\n",
    "    )\n",
    "\n",
    "    # keep only 90 % lowest-entropy pixels\n",
    "    k        = int((1 - TOP_FRAC) * H * W)\n",
    "    thresh   = np.partition(ent_map.flatten(), k)[k]        # kth smallest\n",
    "    valid    = (ent_map <= thresh).astype(np.uint8)\n",
    "\n",
    "    dice_masked.append(\n",
    "        masked_dice(torch.from_numpy(pred_bin)[None,None],\n",
    "                    gt, torch.from_numpy(valid)[None,None])\n",
    "        .item()\n",
    "    )\n",
    "\n",
    "dice_full   = np.array(dice_full)\n",
    "dice_masked = np.array(dice_masked)\n",
    "\n",
    "# paired t-test\n",
    "t_stat, p_val = ttest_rel(dice_full, dice_masked)\n",
    "\n",
    "print(f\"\\nFull-image Dice  :  {dice_full.mean():.4f} ± {dice_full.std():.4f}\")\n",
    "print(f\"Masked Dice      :  {dice_masked.mean():.4f} ± {dice_masked.std():.4f}\")\n",
    "print(f\"Paired t-test    :  t = {t_stat:.3f}   p = {p_val:.4f}\")\n",
    "\n",
    "# box-plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.boxplot([dice_full, dice_masked],\n",
    "            labels=[\"Full\", \"Masked\"], showmeans=True)\n",
    "plt.ylabel(\"Dice score\")\n",
    "plt.title(f\"Effect of removing top 10 % uncertain pixels  |  N={N_SAMPLES}\")\n",
    "plt.grid(axis=\"y\", alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visual demo – 10 % highest-entropy pixels removed\n",
    "random.seed(8)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, gt = test_dataset[idx]\n",
    "img_b   = img.unsqueeze(0).to(device)\n",
    "\n",
    "# MC-dropout entropy\n",
    "mean_pred, _,  ent_map, *_ = mc_dropout_predict(\n",
    "    model, img_b,\n",
    "    num_samples    = N_SAMPLES,      # 30\n",
    "    return_entropy = True\n",
    ")\n",
    "\n",
    "pred_bin = (mean_pred > THR).astype(np.uint8)\n",
    "\n",
    "# build valid-mask (keep 90 % lowest entropy)\n",
    "H, W  = ent_map.shape\n",
    "k     = int((1 - TOP_FRAC) * H * W)\n",
    "thr_e = np.partition(ent_map.flatten(), k)[k]\n",
    "valid = (ent_map <= thr_e).astype(np.uint8)\n",
    "\n",
    "# figure ---------------------------------------------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "axes[0,0].imshow(np.transpose(img.cpu().numpy(), (1,2,0)), cmap='gray')\n",
    "axes[0,0].imshow(pred_bin, alpha=0.3, cmap='Reds')\n",
    "axes[0,0].set_title(\"Baseline prediction\"); axes[0,0].axis('off')\n",
    "\n",
    "im = axes[0,1].imshow(ent_map, cmap='jet', vmin=0, vmax=np.log(2.0))\n",
    "axes[0,1].set_title(\"Predictive entropy\");  axes[0,1].axis('off')\n",
    "plt.colorbar(im, ax=axes[0,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "axes[1,0].imshow(valid, cmap='Greens')\n",
    "axes[1,0].set_title(\"Valid mask (90 % kept)\"); axes[1,0].axis('off')\n",
    "\n",
    "removed = np.where(valid==0, 1, np.nan)      # red overlay on removed\n",
    "axes[1,1].imshow(pred_bin, cmap='gray')\n",
    "axes[1,1].imshow(removed, cmap='autumn', alpha=0.7)\n",
    "axes[1,1].set_title(\"Removed pixels (red)\"); axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, gt = test_dataset[idx]\n",
    "img_b   = img.unsqueeze(0).to(device)\n",
    "\n",
    "# MC-dropout entropy\n",
    "mean_pred, _,  ent_map, *_ = mc_dropout_predict(\n",
    "    model, img_b,\n",
    "    num_samples    = N_SAMPLES,      # 30\n",
    "    return_entropy = True\n",
    ")\n",
    "\n",
    "pred_bin = (mean_pred > THR).astype(np.uint8)\n",
    "\n",
    "# build valid-mask (keep 90 % lowest entropy)\n",
    "H, W  = ent_map.shape\n",
    "k     = int((1 - TOP_FRAC) * H * W)\n",
    "thr_e = np.partition(ent_map.flatten(), k)[k]\n",
    "valid = (ent_map <= thr_e).astype(np.uint8)\n",
    "\n",
    "# figure ---------------------------------------------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "axes[0,0].imshow(np.transpose(img.cpu().numpy(), (1,2,0)), cmap='gray')\n",
    "axes[0,0].imshow(pred_bin, alpha=0.3, cmap='Reds')\n",
    "axes[0,0].set_title(\"Baseline prediction\"); axes[0,0].axis('off')\n",
    "\n",
    "im = axes[0,1].imshow(ent_map, cmap='jet', vmin=0, vmax=np.log(2.0))\n",
    "axes[0,1].set_title(\"Predictive entropy\");  axes[0,1].axis('off')\n",
    "plt.colorbar(im, ax=axes[0,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "axes[1,0].imshow(valid, cmap='Greens')\n",
    "axes[1,0].set_title(\"Valid mask (90 % kept)\"); axes[1,0].axis('off')\n",
    "\n",
    "removed = np.where(valid==0, 1, np.nan)      # red overlay on removed\n",
    "axes[1,1].imshow(pred_bin, cmap='gray')\n",
    "axes[1,1].imshow(removed, cmap='autumn', alpha=0.7)\n",
    "axes[1,1].set_title(\"Removed pixels (red)\"); axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, gt = test_dataset[idx]\n",
    "img_b   = img.unsqueeze(0).to(device)\n",
    "\n",
    "# MC-dropout entropy\n",
    "mean_pred, _, ent_map = mc_dropout_predict(\n",
    "    model, img_b,\n",
    "    num_samples    = N_SAMPLES,      # 30\n",
    "    return_entropy = True            # returns (mean, var, entropy, …)\n",
    ")\n",
    "\n",
    "pred_bin = (mean_pred > THR).astype(np.uint8)\n",
    "\n",
    "# Dice on full image\n",
    "dice_full = dice_score(\n",
    "    torch.from_numpy(pred_bin)[None, None].float(),   # (1,1,H,W)\n",
    "    gt.unsqueeze(0).float(),                          # (1,1,H,W)\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "# Build valid-mask – keep 90 % lowest-entropy pixels\n",
    "H, W  = ent_map.shape\n",
    "k     = int((1 - TOP_FRAC) * H * W)\n",
    "thr_e = np.partition(ent_map.flatten(), k)[k]\n",
    "valid = (ent_map <= thr_e).astype(np.uint8)\n",
    "\n",
    "# Dice on valid pixels only\n",
    "dice_masked = masked_dice(\n",
    "    torch.from_numpy(pred_bin)[None, None],\n",
    "    gt.unsqueeze(0),\n",
    "    torch.from_numpy(valid)[None, None]\n",
    ")\n",
    "\n",
    "# --------------------------- figure ------------------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "axes[0,0].imshow(np.transpose(img.cpu().numpy(), (1,2,0)), cmap='gray')\n",
    "axes[0,0].imshow(pred_bin, alpha=0.3, cmap='Reds')\n",
    "axes[0,0].set_title(\"Baseline prediction\"); axes[0,0].axis('off')\n",
    "\n",
    "im = axes[0,1].imshow(ent_map, cmap='jet', vmin=0, vmax=np.log(2.0))\n",
    "axes[0,1].set_title(\"Predictive entropy\");  axes[0,1].axis('off')\n",
    "plt.colorbar(im, ax=axes[0,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "axes[1,0].imshow(valid, cmap='Greens')\n",
    "axes[1,0].set_title(\"Valid mask (90 % kept)\"); axes[1,0].axis('off')\n",
    "\n",
    "removed = np.where(valid == 0, 1, np.nan)          # red overlay\n",
    "axes[1,1].imshow(pred_bin, cmap='gray')\n",
    "axes[1,1].imshow(removed, cmap='autumn', alpha=0.7)\n",
    "axes[1,1].set_title(\"Removed pixels (red)\"); axes[1,1].axis('off')\n",
    "\n",
    "plt.figtext(0.5, 0,\n",
    "       f\"Dice score: Dice full: {dice_full:.3f}   \"\n",
    "    f\"Dice masked: {dice_masked:.3f}\",\n",
    " fontsize=12, ha='center', va='top'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 : Compare SE (no aug) vs. SENOAUG (flip + rot) on epistemic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SE       = \"/segmentation/unet_segmentationSE.pth\"\n",
    "PATH_SENOAUG  = \"/segmentation/unet_segmentationSENOAUG.pth\"\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_unet(path):\n",
    "    net = UNetWithDropout(in_channels=3, out_channels=1).to(device)\n",
    "    net.load_state_dict(torch.load(path, map_location=device))\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "model_no_aug      = load_unet(PATH_SENOAUG)\n",
    "model_aug     = load_unet(PATH_SE)\n",
    "\n",
    "# ------------------------------------------------------------------ eval helper\n",
    "def eval_dice_entropy(model, n_mc=30, thr=0.4):\n",
    "    dice_list, ent_list = [], []\n",
    "    for img, gt in test_loader:\n",
    "        img = img.to(device)\n",
    "        mean_pred, _, ent_map = mc_dropout_predict(\n",
    "            model, img,\n",
    "            num_samples    = n_mc,\n",
    "            return_entropy = True\n",
    "        )\n",
    "        pred_bin = (mean_pred > thr).astype(np.uint8)\n",
    "        dice_val = dice_score(\n",
    "            torch.from_numpy(pred_bin)[None,None].float(),\n",
    "            gt.float(), threshold=0.5\n",
    "        )\n",
    "        dice_list.append(dice_val)\n",
    "        ent_list .append(ent_map.mean())\n",
    "    return np.array(dice_list), np.array(ent_list)\n",
    "\n",
    "dice_no_aug , ent_no_aug  = eval_dice_entropy(model_no_aug)\n",
    "dice_aug, ent_aug = eval_dice_entropy(model_aug)\n",
    "\n",
    "# --------------------------- statistics --------------------------\n",
    "t_dice, p_dice = ttest_rel(dice_no_aug,  dice_aug)\n",
    "t_ent , p_ent  = ttest_rel(ent_no_aug,   ent_aug)\n",
    "\n",
    "print(\"\\n===  RESULTS  ===\")\n",
    "print(f\"Dice  No AUG       : {dice_no_aug.mean():.4f} ± {dice_no_aug.std():.4f}\")\n",
    "print(f\"Dice  With AUG  : {dice_aug.mean():.4f} ± {dice_aug.std():.4f}\")\n",
    "print(f\"Paired t-test Dice   p = {p_dice:.4e}\")\n",
    "\n",
    "print(f\"\\nEntropy  No AUG    : {ent_no_aug.mean():.4f} ± {ent_no_aug.std():.4f}\")\n",
    "print(f\"Entropy  With AUG   : {ent_aug.mean():.4f} ± {ent_aug.std():.4f}\")\n",
    "print(f\"Paired t-test entropy p = {p_ent:.4e}\")\n",
    "\n",
    "# --------------------------- bar plots ---------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "axes[0].bar([\"No AUG\",\"With AUG\"], [dice_no_aug.mean(), dice_aug.mean()],\n",
    "            yerr=[dice_no_aug.std(), dice_aug.std()], capsize=6)\n",
    "axes[0].set_title(\"Average Dice\"); axes[0].set_ylabel(\"Dice score\")\n",
    "\n",
    "axes[1].bar([\"No AUG\",\"With AUG\"], [ent_no_aug.mean(), ent_aug.mean()],\n",
    "            yerr=[ent_no_aug.std(), ent_aug.std()], capsize=6, color=\"orange\")\n",
    "axes[1].set_title(\"Mean predictive entropy\"); axes[1].set_ylabel(\"bits\")\n",
    "\n",
    "plt.suptitle(\"Effect of TRAINING augmentation (flip + rotations)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)\n",
    "idx = random.randrange(len(test_dataset))\n",
    "img, _ = test_dataset[idx]\n",
    "img_b  = img.unsqueeze(0).to(device)\n",
    "\n",
    "# predictions + entropy\n",
    "prob_no, _, ent_no = mc_dropout_predict(\n",
    "    model_no_aug, img_b, num_samples=30, return_entropy=True)\n",
    "prob_aug, _, ent_aug = mc_dropout_predict(\n",
    "    model_aug, img_b, num_samples=30, return_entropy=True)\n",
    "\n",
    "pred_no  = (prob_no  > 0.4).astype(np.uint8)\n",
    "pred_aug = (prob_aug > 0.4).astype(np.uint8)\n",
    "diff_ent = ent_aug - ent_no\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(11, 7))\n",
    "\n",
    "# row 1 – predictions\n",
    "ax[0,0].imshow(np.transpose(img.cpu().numpy(),(1,2,0)), cmap=\"gray\")\n",
    "ax[0,0].set_title(\"Original\"); ax[0,0].axis('off')\n",
    "\n",
    "ax[0,1].imshow(pred_no, cmap=\"gray\"); ax[0,1].set_title(\"Prediction  • No AUG\"); ax[0,1].axis('off')\n",
    "ax[0,2].imshow(pred_aug, cmap=\"gray\"); ax[0,2].set_title(\"Prediction  • With AUG\"); ax[0,2].axis('off')\n",
    "\n",
    "# row 2 – entropies\n",
    "for j,(emap,ttl) in enumerate([(ent_no,\"Entropy  • No AUG\"),\n",
    "                               (ent_aug,\"Entropy  • With AUG\"),\n",
    "                               (diff_ent,\"Δ Entropy  (With UG–No AUG)\")]):\n",
    "    cmap = 'jet' if j<2 else 'bwr'\n",
    "    vmin,vmax = (0, np.log(2.0)) if j<2 else (-0.3,0.3)\n",
    "    im = ax[1,j].imshow(emap, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax[1,j].set_title(ttl); ax[1,j].axis('off'); plt.colorbar(im, ax=ax[1,j], fraction=0.046)\n",
    "\n",
    "plt.suptitle(f\"Effect of training augmentation on epistemic uncertainty\", y=1)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-example figure\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# parameters \n",
    "N_EX     = 30\n",
    "N_MC     = NUM_SAMPLES     \n",
    "THR_BIN  = PRED_THRESH      \n",
    "random.seed(36)\n",
    "\n",
    "def overlay(img_rgb, mask, color):\n",
    "    out   = img_rgb.copy()\n",
    "    edges = cv2.Canny((mask*255).astype(np.uint8), 100, 200)\n",
    "    out[edges>0] = color\n",
    "    return out\n",
    "\n",
    "idx_pool = list(range(len(test_dataset))); random.shuffle(idx_pool)\n",
    "sel_idx  = idx_pool[:N_EX]\n",
    "\n",
    "for ex, idx in enumerate(sel_idx, 1):\n",
    "    img, gt = test_dataset[idx]\n",
    "    img_b   = img.unsqueeze(0).to(device)\n",
    "    rtg     = np.transpose(img.cpu().numpy(), (1,2,0))\n",
    "    mGT     = gt.squeeze().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    # baseline\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        log_det = model(img_b)\n",
    "    prob_det = torch.sigmoid(log_det)[0,0].cpu().numpy()\n",
    "    mDET     = (prob_det > THR_BIN).astype(np.uint8)\n",
    "\n",
    "    # MC-dropout\n",
    "    pm, _, ent = mc_dropout_predict(\n",
    "        model, img_b, num_samples=N_MC, return_entropy=True)\n",
    "    mTTD = (pm > THR_BIN).astype(np.uint8)\n",
    "\n",
    "    # overlays\n",
    "    rgb_det = overlay(overlay(rtg, mDET, (0,1,0)), mGT, (1,1,0))\n",
    "    rgb_ttd = overlay(overlay(rtg, mTTD, (0,1,1)), mGT, (1,1,0))\n",
    "\n",
    "    # layout  (extra room on the right for colour-bar + legend)\n",
    "    gs = dict(width_ratios=[1,1,0.07], height_ratios=[1,0.15,1],\n",
    "              wspace=0.02, hspace=0.04)\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(9.2, 8), gridspec_kw=gs)\n",
    "\n",
    "    # row-1 ----------------------------------------------------------------\n",
    "    ax[0,0].imshow(rtg, cmap='gray');   ax[0,0].set_title(\"Original X-ray\")\n",
    "    ax[0,1].imshow(rgb_det);            ax[0,1].set_title(\"Baseline + GT\")\n",
    "    for a in ax[0,:2]: a.axis('off')   \n",
    "    ax[0,2].axis('off')\n",
    "\n",
    "    # ------------------ gap between lines ---------------------\n",
    "    for a in ax[1,:]:\n",
    "        a.axis('off')  \n",
    "    # row-2 ----------------------------------------------------------------\n",
    "    im = ax[2,0].imshow(ent, cmap='jet', vmin=0, vmax=np.log(2.0))\n",
    "    ax[2,0].set_title(\"Epistemic unertainty\");           ax[2,0].axis('off')\n",
    "    ax[2,1].imshow(rgb_ttd);                    ax[2,1].set_title(\"TTD + GT\")\n",
    "    ax[2,1].axis('off');                       ax[2,2].axis('off')\n",
    "\n",
    "    # color-bar\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), shrink=0.75)\n",
    "    cbar.set_label(\"Entropy  H(p̂)  [bits]\")\n",
    "\n",
    "    # legend – placed **above** colour-bar (bbox y > 1)\n",
    "    handles = [mpatches.Patch(color='yellow', label='Ground truth'),\n",
    "               mpatches.Patch(color='lime',   label='Baseline prediction'),\n",
    "               mpatches.Patch(color='cyan',   label='TTD prediction')]\n",
    "    fig.legend(handles=handles, loc='upper right',\n",
    "               bbox_to_anchor=(0.97, 0.90), frameon=True)\n",
    "\n",
    "    plt.tight_layout(rect=[0,0,0.94,0.97])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9 : Entropy–penalised loss (epistemic consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Training with entropy–penalised loss (epistemic consistency)\n",
    "\n",
    "\n",
    "# ---------------- hyper–parameters --------------------------------------\n",
    "K_PASS       = 8    # dropout forward passes per mini-batch\n",
    "LAMBDA_ENT   = 0.2     # weight of entropy penalty\n",
    "N_EPOCHS     = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------- model, optimiser, base-loss ------------------------------\n",
    "model_unc = UNetWithDropout(in_channels=3, out_channels=1).to(device)\n",
    "base_crit  = CombinedLoss(weight_dice=0.7, weight_bce=0.3)\n",
    "optim      = torch.optim.Adam(model_unc.parameters(), lr=1e-4)\n",
    "\n",
    "# ------------- helper : entropy of mean-probabilities --------------------\n",
    "def entropy_from_mean(p_mean, eps=1e-12):\n",
    "    p_mean = p_mean.clamp(eps, 1. - eps)\n",
    "    return -(p_mean * p_mean.log() + (1 - p_mean) * (1 - p_mean).log())\n",
    "\n",
    "# ------------- training loop --------------------------------------------\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model_unc.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, gts in tqdm(train_loader, desc=f\"Epoch {epoch}/{N_EPOCHS}\"):\n",
    "        imgs, gts = imgs.to(device), gts.to(device)\n",
    "\n",
    "        # K stochastic forward passes ------------------------------------\n",
    "        probs = []\n",
    "        for _ in range(K_PASS):\n",
    "            logits = model_unc(imgs)            # dropout active\n",
    "            probs.append(torch.sigmoid(logits))\n",
    "        probs = torch.stack(probs, dim=0)        # [K,B,1,H,W]\n",
    "\n",
    "        # mean prediction & base loss ------------------------------------\n",
    "        p_mean = probs.mean(dim=0)               # [B,1,H,W]\n",
    "        base_loss = base_crit(p_mean, gts)\n",
    "\n",
    "        # entropy penalty -------------------------------------------------\n",
    "        ent_map   = entropy_from_mean(p_mean)\n",
    "        ent_loss  = ent_map.mean()\n",
    "\n",
    "        loss = base_loss + LAMBDA_ENT * ent_loss\n",
    "\n",
    "        # backward --------------------------------------------------------\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    dice_val = torch.tensor(dice_vals).mean().item()\n",
    "    print(f\"Epoch {epoch:02d} | loss = {running_loss/len(train_loader):.4f} | val-Dice={dice_val:.4f}\")\n",
    "\n",
    "print(\"Finished uncertainty-aware training.  You can now evaluate the new model.\")\n",
    "torch.save(model.state_dict(), \"segmentation/unet_segmentationUncertainty.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "# ------------ hyper-params & loaders ------------------------------------\n",
    "K_PASS       = 8\n",
    "LAMBDA_ENT   = 0.2\n",
    "N_EPOCHS     = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_unc = UNetWithDropout(in_channels=3, out_channels=1).to(device)\n",
    "base_crit  = CombinedLoss(weight_dice=0.7, weight_bce=0.3)\n",
    "optim      = torch.optim.Adam(model_unc.parameters(), lr=1e-4)\n",
    "\n",
    "def entropy_from_mean(p_mean, eps=1e-12):\n",
    "    p = p_mean.clamp(eps, 1. - eps)\n",
    "    return -(p * p.log() + (1 - p) * (1 - p).log())\n",
    "\n",
    "# ---------------- training loop -----------------------------------------\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model_unc.train()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    for imgs, gts in tqdm(train_loader, desc=f\"Epoch {epoch}/{N_EPOCHS}\"):\n",
    "        imgs, gts = imgs.to(device), gts.to(device)\n",
    "\n",
    "        # K stochastic passes\n",
    "        probs = torch.stack(\n",
    "            [torch.sigmoid(model_unc(imgs)) for _ in range(K_PASS)], dim=0\n",
    "        )                                # [K,B,1,H,W]\n",
    "\n",
    "        p_mean   = probs.mean(dim=0)\n",
    "        base_loss = base_crit(p_mean, gts)\n",
    "        ent_loss  = entropy_from_mean(p_mean).mean()\n",
    "        loss      = base_loss + LAMBDA_ENT * ent_loss\n",
    "\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "        loss_accum += loss.item()\n",
    "\n",
    "    # ---------- validation Dice -----------------------------------------\n",
    "    model_unc.eval()\n",
    "    dice_vals = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, gts in val_loader:\n",
    "            imgs, gts = imgs.to(device), gts.to(device)\n",
    "            p_mean = torch.sigmoid(model_unc(imgs))\n",
    "            dice_vals.append(\n",
    "                dice_score(p_mean, gts, threshold=0.5)\n",
    "            )\n",
    "    dice_val = torch.tensor(dice_vals).mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | loss={loss_accum/len(train_loader):.4f}\"\n",
    "          f\" | val-Dice={dice_val:.4f}\")\n",
    "    if early_stopping.check_stop(avg_val_loss):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        torch.save(model.state_dict(), \"segmentation/unet_segmentationSEUncertainty.pth\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished; model `model_unc` now contains epistemic penalty.\")\n",
    "torch.save(model.state_dict(), \"segmentation/unet_segmentationUncertainty.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy-penalised training + correct val-Dice + early stopping\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "# hyper-params -------------------------------------------------------------\n",
    "K_PASS     = 8\n",
    "LAMBDA_ENT = 0.2\n",
    "N_EPOCHS   = 30\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_unc  = UNetWithDropout(in_channels=3, out_channels=1).to(device)\n",
    "base_crit  = CombinedLoss(weight_dice=0.7, weight_bce=0.3)\n",
    "optim      = torch.optim.Adam(model_unc.parameters(), lr=1e-4)\n",
    "\n",
    "def entropy_from_mean(p_mean, eps=1e-12):\n",
    "    p = p_mean.clamp(eps, 1. - eps)\n",
    "    return -(p * p.log() + (1 - p) * (1 - p).log())\n",
    "\n",
    "# training loop ------------------------------------------------------------\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model_unc.train()\n",
    "    train_loss_accum = 0.0\n",
    "\n",
    "    for imgs, gts in tqdm(train_loader, desc=f\"Epoch {epoch}/{N_EPOCHS}\"):\n",
    "        imgs, gts = imgs.to(device), gts.to(device)\n",
    "\n",
    "        # K stochastic forward passes (dropout ON)\n",
    "        probs = torch.stack(\n",
    "            [torch.sigmoid(model_unc(imgs)) for _ in range(K_PASS)], dim=0\n",
    "        )                                   # [K,B,1,H,W]\n",
    "\n",
    "        p_mean   = probs.mean(dim=0)\n",
    "        base_loss = base_crit(p_mean, gts)\n",
    "        ent_loss  = entropy_from_mean(p_mean).mean()\n",
    "        loss      = base_loss + LAMBDA_ENT * ent_loss\n",
    "\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "        train_loss_accum += loss.item()\n",
    "\n",
    "    # ---------- validation ------------------------------------------------\n",
    "    model_unc.eval()\n",
    "    val_dice_vals, val_loss_vals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, gts in val_loader:\n",
    "            imgs, gts = imgs.to(device), gts.to(device)\n",
    "\n",
    "            logits = model_unc(imgs)                           # dropout OFF\n",
    "            probs  = torch.sigmoid(logits)\n",
    "\n",
    "            val_loss_vals.append(base_crit(probs, gts).item())\n",
    "\n",
    "\n",
    "            val_dice_vals.append(dice_score(logits, gts, threshold=0.5))\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss_vals)                     \n",
    "    avg_val_dice = np.mean(val_dice_vals)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train-loss={train_loss_accum/len(train_loader):.4f}\"\n",
    "          f\" | val-loss={avg_val_loss:.4f} | val-Dice={avg_val_dice:.4f}\")\n",
    "\n",
    "    # early stopping -------------------------------------------------------\n",
    "    if early_stopping.check_stop(avg_val_loss):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        torch.save(model_unc.state_dict(),\n",
    "                   \"segmentation/unet_segmentation_uncertaintySE.pth\")    \n",
    "        break\n",
    "\n",
    "# save final state (if ES nevyplo skôr) ------------------------------------\n",
    "torch.save(model_unc.state_dict(),\n",
    "           \"segmentation/unet_segmentation_uncertainty.pth\")\n",
    "print(\"Training finished; model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
